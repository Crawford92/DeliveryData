{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5258c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES #\n",
    "import pandas as pd\n",
    "import pygsheets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e77e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS #\n",
    "\n",
    "#Set to 1 to print diagnostic info\n",
    "debugFlag = 0\n",
    "#Set to 1 to pull info from RawNames sheet, join GPS, update, and export\n",
    "updateGPSFlag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fb4ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused worksheet detected:  <Worksheet 'Import' index:6>\n",
      "Unused worksheet detected:  <Worksheet 'Lifetime Deliveries' index:7>\n",
      "Unused worksheet detected:  <Worksheet 'Days Old' index:8>\n",
      "Unused worksheet detected:  <Worksheet 'Weeks Old' index:9>\n"
     ]
    }
   ],
   "source": [
    "# IMPORT DATA #\n",
    "\n",
    "gc = pygsheets.authorize(service_file='file.json')\n",
    "sh = gc.open('DashData')\n",
    "\n",
    "if debugFlag:\n",
    "    for x in sh:\n",
    "        print(x)\n",
    "\n",
    "for x in sh:\n",
    "    title = x.title\n",
    "    if(title==\"DeliveriesOld\"):\n",
    "        import_sheets_OldDelDF = x.get_as_df()\n",
    "    elif(title==\"Deliveries\"):\n",
    "        import_sheets_DelDF = x.get_as_df()\n",
    "    elif(title==\"Days\"):\n",
    "        import_sheets_DaysDF = x.get_as_df()\n",
    "    elif(title==\"Weeks\"):\n",
    "        import_sheets_WeeksDF = x.get_as_df()\n",
    "    elif(title==\"RawNames\"):\n",
    "        import_sheets_RawNamesDF = x.get_as_df()\n",
    "    elif(title==\"GPS\"):\n",
    "        #for import\n",
    "        import_sheets_GPSDF = x.get_as_df()\n",
    "        #for export\n",
    "        import_wks_gps = x\n",
    "    else:\n",
    "        print(\"Unused worksheet detected: \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ac86bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for disjoint cells, one for import, one for reset to import value (for ease of testing)\n",
    "\n",
    "#DF from sheets\n",
    "sheets_OldDelDF   = import_sheets_OldDelDF.copy()\n",
    "sheets_DelDF      = import_sheets_DelDF.copy()\n",
    "sheets_DaysDF     = import_sheets_DaysDF.copy()\n",
    "sheets_WeeksDF    = import_sheets_WeeksDF.copy()\n",
    "sheets_RawNamesDF = import_sheets_RawNamesDF.copy()\n",
    "sheets_GPSDF      = import_sheets_GPSDF.copy()\n",
    "\n",
    "#Sheets, ah right can't use copy on a worksheet object, might need an alternate\n",
    "#wks_gps           = import_wks_gps.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8494cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS #\n",
    "\n",
    "# TODO: develop function to take avg distance and duration for stacked orders\n",
    "def processStacks(df):\n",
    "    return df\n",
    "\n",
    "def dollars_to_floats(df, columns):\n",
    "    for x in columns:\n",
    "        df[x] = df[x].str.replace('$', '')\n",
    "        df[x] = df[x].str.replace(',', '')\n",
    "        df[x] = df[x].astype(float)\n",
    "        \n",
    "def hours_to_floats(df, columns):\n",
    "    for x in columns:\n",
    "        for index, row in df.iterrows():\n",
    "            active = row[x]\n",
    "            activeH = float(active[:active.index(\":\")])\n",
    "            activeM = round(float(active[active.index(\":\")+1:active.index(\":\")+3])/60, 2)\n",
    "            active = activeH + activeM\n",
    "            df.at[index, x] = active\n",
    "        df[x] = df[x].astype(float)\n",
    "\n",
    "def getStartHour(df):\n",
    "    df[\"Start Hour\"] = -1\n",
    "    for index, row in df.iterrows():\n",
    "        startHour = row[\"Start Time\"]\n",
    "        startHour = int(startHour[:startHour.index(\":\")])\n",
    "        df.at[index, \"Start Hour\"] = startHour\n",
    "\n",
    "def addStartAndEndDate(df):\n",
    "    lastRow = df.iloc[-1]\n",
    "    df = df[:-1]\n",
    "    df[\"Start Date\"] = -1\n",
    "    df[\"End Date\"]   = -1\n",
    "    for index, row in df.iterrows():\n",
    "        tempVar = row[\"Dates\"]\n",
    "        tempVar1 = tempVar[:tempVar.index(\" \")]\n",
    "        tempVar2 = tempVar[tempVar.index(\"- \")+2:]\n",
    "        #print(tempVar1, tempVar2)\n",
    "        df.at[index, \"Start Date\"] = tempVar1\n",
    "        df.at[index, \"End Date\"] = tempVar2\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    #print(list(df.columns.values))\n",
    "    df = pd.concat([df, lastRow.to_frame().T], ignore_index=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #print(list(df.columns.values))\n",
    "    print(df.dtypes)\n",
    "    return df\n",
    "\n",
    "#Currently only valid for 2023\n",
    "def date_to_datetime(df, column):\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, column] = pd.Timestamp('2023/' + row[column])\n",
    "    df[column] = pd.to_datetime(df[column])\n",
    "    #print(\"TEST\")\n",
    "    #print(df.dtypes)\n",
    "        \n",
    "        \n",
    "#Currently only valid for 2023\n",
    "# Currently unused function\n",
    "# Would we actually want this to have different dates? it's really the date the shift started\n",
    "# So maybe, keep date same, but certain analysis might require that difference to be handled\n",
    "def time_to_timestamp(df, DateCol, TimeCols):\n",
    "    for index, row in df.iterrows():\n",
    "        df.at[index, TimeCols[0]] = pd.Timestamp('2023/' + row[DateCol] + \" \" + row[TimeCols[0]])\n",
    "        df.at[index, TimeCols[1]] = pd.Timestamp('2023/' + row[DateCol] + \" \" + row[TimeCols[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0c388753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE GPS WORKSHEET #\n",
    "\n",
    "if updateGPSFlag:\n",
    "    testingDF = sheets_RawNamesDF.copy()\n",
    "    testingDF = testingDF.drop_duplicates(subset=[\"Restaurant Name\", \"RAW DATA\"])\n",
    "    testingDF = testingDF[[\"Restaurant Name\", \"RAW DATA\", \"Note\"]]\n",
    "    newNamesDF = testingDF.merge(sheets_GPSDF, how='left')\n",
    "    for index, row in newNamesDF.iterrows():\n",
    "        if isinstance(row['Address'], float):\n",
    "            if not row['Note'] == \"\":\n",
    "                newNamesDF.at[index, \"Address\"] = row[\"Note\"]\n",
    "            else:\n",
    "                print(\"Note not found for:\", index)\n",
    "        if (row['Address'] == \"\"):\n",
    "            newNamesDF.at[index, \"Resolved\"] = \"0\"\n",
    "        else:\n",
    "            newNamesDF.at[index, \"Resolved\"] = \"1\"\n",
    "    newNamesDF = newNamesDF.drop(columns=[\"Note\"])\n",
    "    import_wks_gps.set_dataframe(newNamesDF,(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3a024fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START PROCESSING #\n",
    "#sheets_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7b7fc8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE DELIVERY DATA #\n",
    "#sheets_frames = [sheets_OldDelDF, sheets_DelDF]\n",
    "#sheets_result = pd.concat(sheets_frames)\n",
    "#sheets_result = sheets_OldDelDF.append(sheets_DelDF)\n",
    "#sheets_result = pd.concat([sheets_OldDelDF, sheets_DelDF], sort=False)\n",
    "#sheets_result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9d5409be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30941/3616702342.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[x] = df[x].str.replace('$', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates              object\n",
      "Base              float64\n",
      "Tip               float64\n",
      "Adj pay           float64\n",
      "Other             float64\n",
      "Pre Total         float64\n",
      "Total             float64\n",
      "Active            float64\n",
      "Dash              float64\n",
      "Deliveries          int64\n",
      "Total Mileage      object\n",
      "Active Mileage     object\n",
      "Version            object\n",
      "WeekID             object\n",
      "dtype: object\n",
      "Dates              object\n",
      "Base              float64\n",
      "Tip               float64\n",
      "Adj pay           float64\n",
      "Other             float64\n",
      "Pre Total         float64\n",
      "Total             float64\n",
      "Active            float64\n",
      "Dash              float64\n",
      "Deliveries          int64\n",
      "Total Mileage      object\n",
      "Active Mileage     object\n",
      "Version            object\n",
      "WeekID             object\n",
      "Start Date         object\n",
      "End Date           object\n",
      "dtype: object\n",
      "Dates             object\n",
      "Base              object\n",
      "Tip               object\n",
      "Adj pay           object\n",
      "Other             object\n",
      "Pre Total         object\n",
      "Total             object\n",
      "Active            object\n",
      "Dash              object\n",
      "Deliveries        object\n",
      "Total Mileage     object\n",
      "Active Mileage    object\n",
      "Version           object\n",
      "WeekID            object\n",
      "Start Date        object\n",
      "End Date          object\n",
      "dtype: object\n",
      "Dates             object\n",
      "Base              object\n",
      "Tip               object\n",
      "Adj pay           object\n",
      "Other             object\n",
      "Pre Total         object\n",
      "Total             object\n",
      "Active            object\n",
      "Dash              object\n",
      "Deliveries        object\n",
      "Total Mileage     object\n",
      "Active Mileage    object\n",
      "Version           object\n",
      "WeekID            object\n",
      "Start Date        object\n",
      "End Date          object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30941/3616702342.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[x] = df[x].str.replace('$', '')\n",
      "/tmp/ipykernel_30941/3616702342.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[x] = df[x].str.replace('$', '')\n",
      "/tmp/ipykernel_30941/3616702342.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Start Date\"] = -1\n",
      "/tmp/ipykernel_30941/3616702342.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"End Date\"]   = -1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type float which has no callable rint method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'rint'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[286], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pretotalactiveratio refers to weekly pre adjustment pay / time spent on deliveries\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(sheets_WeeksDF\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m---> 21\u001b[0m sheets_WeeksDF[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretotalactiveratio\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheets_WeeksDF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPre Total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43msheets_WeeksDF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mActive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py:1658\u001b[0m, in \u001b[0;36mNDFrame.__round__\u001b[0;34m(self, decimals)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__round__\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, decimals: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecimals\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__round__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:2602\u001b[0m, in \u001b[0;36mSeries.round\u001b[0;34m(self, decimals, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;124;03mRound each value in a Series to the given number of decimals.\u001b[39;00m\n\u001b[1;32m   2572\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;124;03mdtype: float64\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2601\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_round(args, kwargs)\n\u001b[0;32m-> 2602\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2603\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(result, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2605\u001b[0m )\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type float which has no callable rint method"
     ]
    }
   ],
   "source": [
    "# FUNCTIONS AND OTHER SHORT PROCESSING STEPS #\n",
    "\n",
    "\n",
    "#All\n",
    "dollars_to_floats(sheets_OldDelDF, [\"Total\", \"Base\", \"Tip\", \"Peak Bonus\"])\n",
    "#V3 only\n",
    "dollars_to_floats(sheets_DelDF, [\"Total\", \"Base\", \"Tip\", \"Peak Bonus\"])\n",
    "getStartHour(sheets_DelDF)\n",
    "#Days\n",
    "dollars_to_floats(sheets_DaysDF, [\"Total\", \"Base\", \"Tip\"])\n",
    "hours_to_floats(sheets_DaysDF, [\"Dash\", \"Active\"])\n",
    "date_to_datetime(sheets_DaysDF, \"Date\")\n",
    "sheets_DaysDF[\"Day of Week\"] = sheets_DaysDF[\"Date\"].dt.day_name()\n",
    "#Weeks\n",
    "dollars_to_floats(sheets_WeeksDF, [\"Total\", \"Base\", \"Tip\", \"Adj pay\", \"Other\", \"Pre Total\"])\n",
    "hours_to_floats(sheets_WeeksDF, [\"Dash\", \"Active\"])\n",
    "#print(sheets_WeeksDF.dtypes)\n",
    "sheets_WeeksDF = addStartAndEndDate(sheets_WeeksDF)\n",
    "# pretotalactiveratio refers to weekly pre adjustment pay / time spent on deliveries\n",
    "#print(sheets_WeeksDF.dtypes)\n",
    "sheets_WeeksDF[\"pretotalactiveratio\"] = round(sheets_WeeksDF[\"Pre Total\"]/sheets_WeeksDF[\"Active\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_WeeksDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f07fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN ADJ PAY SECTION #\n",
    "\n",
    "# This establishes the estimated pay adjustment estimates by delivery, date, and week\n",
    "# This is calculated using time spent delivering and distance approximations\n",
    "# The real adjusted pay is based on weekly values\n",
    "# I suspect the primary cause for difference between estimate and real values stems from \n",
    "#how I estimate mileage vs how the app tracks mileage\n",
    "\n",
    "# DFs are copied to avoid excluding dropped values in later analysis\n",
    "\n",
    "\n",
    "\n",
    "#if debugFlag:\n",
    "#    sheets_DelDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23006e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per delivery estimations\n",
    "#sheets_DelDF1 = sheets_DelDF.copy()\n",
    "\n",
    "#Convert to float if possible, else convert to NaN\n",
    "#sheets_DelDF1[\"Duration\"] = pd.to_numeric(sheets_DelDF1[\"Duration\"], errors='coerce')\n",
    "sheets_DelDF[\"Duration\"] = pd.to_numeric(sheets_DelDF[\"Duration\"])\n",
    "#Drop NaN rows\n",
    "#sheets_DelDF1 = sheets_DelDF1[sheets_DelDF1[\"Duration\"].notna()]\n",
    "#Calculate estimate adjustment per delivery\n",
    "sheets_DelDF[\"Est Adj\"] = (sheets_DelDF[\"Duration\"]/60*18\n",
    "                           + sheets_DelDF[\"Distance\"]*(0.34) ) - sheets_DelDF[\"Base\"]\n",
    "#Duration is over 60 because its value is in minutes, values from day and week are in hours\n",
    "\n",
    "#if debugFlag:\n",
    "#    sheets_DelDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22355259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if debugFlag:\n",
    "#    sheets_DaysDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per day estimations\n",
    "sheets_DaysDF1 = sheets_DaysDF.copy()\n",
    "sheets_DaysDF1 = sheets_DaysDF1[sheets_DaysDF1[\"Version\"] == 3]\n",
    "sheets_DaysDF1[\"Est Adj\"] = (sheets_DaysDF1[\"Active\"]*(18) \n",
    "                            + sheets_DaysDF1[\"Active Mileage\"]*(0.34) ) - sheets_DaysDF1[\"Base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60990d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if debugFlag:\n",
    "#    sheets_DaysDFv3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e45d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per week estimations\n",
    "sheets_WeeksDF1 = sheets_WeeksDF.copy()\n",
    "sheets_WeeksDF1 = sheets_WeeksDF1[sheets_WeeksDF1[\"Version\"] == 3] #totals doesn't have v3 so its dropped\n",
    "sheets_WeeksDF1[\"Est Adj\"] = (sheets_WeeksDF1[\"Active\"]*(18) \n",
    "                            + sheets_WeeksDF1[\"Active Mileage\"]*(0.34) ) - sheets_WeeksDF1[\"Base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appends\n",
    "\n",
    "\n",
    "# Sample values\n",
    "if debugFlag:\n",
    "    print(\"Del DF Sample\\n\")\n",
    "    print(sheets_DelDF.iloc[-1])\n",
    "    print(\"\\nDays DF Sample\\n\")\n",
    "    print(sheets_DaysDF1.iloc[-1])\n",
    "    print(\"\\nWeeks DF Sample\\n\")\n",
    "    print(sheets_WeeksDF1.iloc[-1])\n",
    "    \n",
    "    \n",
    "# END ADJ PAY SECTION #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb67e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(sheets_DelDF.columns.values))\n",
    "print()\n",
    "print(list(sheets_DaysDF.columns.values))\n",
    "print()\n",
    "print(list(sheets_WeeksDF.columns.values))\n",
    "print()\n",
    "print(sheets_DelDF[\"Start Hour\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets_WeeksDF.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTUALLY LETS DO THE RESULTS JOIN HERE\n",
    "\n",
    "sheets_result = pd.concat([sheets_OldDelDF, sheets_DelDF], sort=False)\n",
    "sheets_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#NO okay so we have to concat df1 (v3) to df (v1,2), THIS SHOULD WORK\n",
    "sheets_DaysDF0 = sheets_WeeksDF[sheets_WeeksDF[\"Version\"] != 3]\n",
    "sheets_WeeksDF0 = sheets_WeeksDF[sheets_WeeksDF[\"Version\"] != 3] #THIS PUTS TOTALS IN THE MIDDLE\n",
    "total_S = sheets_WeeksDF0.iloc[-1]\n",
    "sheets_WeeksDF0 = sheets_WeeksDF0[:-1]\n",
    "#okay so what if we grab wksdf0.iloc-1, store, drop, then add it at the end\n",
    "\n",
    "sheets_DaysDF = pd.concat([sheets_DaysDF0, sheets_DaysDF1], sort=False)\n",
    "sheets_DaysDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "sheets_WeeksDF = pd.concat([sheets_WeeksDF0, sheets_WeeksDF1], sort=False)\n",
    "sheets_WeeksDF = pd.concat([sheets_WeeksDF, total_S.to_frame().T], ignore_index=True)\n",
    "sheets_WeeksDF.reset_index(drop=True, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempVar = sheets_WeeksDF[\"Dates\"].iloc[0]\n",
    "tempVar1 = tempVar[:tempVar.index(\" \")]\n",
    "tempVar2 = tempVar[tempVar.index(\"- \")+2:]\n",
    "print(tempVar1, tempVar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END PROCESSING #\n",
    "# START INITIAL EXPLORATION #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of deliveries taken that tip\n",
    "numUntipped = (sheets_result[\"Tip\"] == 0.0).sum()\n",
    "numTipped = (sheets_result[\"Tip\"] != 0.0).sum()\n",
    "percTipped = round((numTipped/(numUntipped+numTipped)), 4)\n",
    "print(percTipped*100, \"% of accepted deliveries included tip\")\n",
    "\n",
    "#Average tip, including and excluding untipped deliveries\n",
    "TipAvg = round(sheets_result[\"Tip\"].mean(), 2)\n",
    "print(\"$\", TipAvg, \"average tip value including orders with no tip\")\n",
    "TipAvg2 = round(sheets_result[sheets_result[\"Tip\"] !=0][\"Tip\"].mean(), 2)\n",
    "print(\"$\", TipAvg2, \"average tip value excluding orders with no tip\")\n",
    "\n",
    "# Overall hourly rate\n",
    "hourlyRate = round(sheets_WeeksDF.iloc[-1][\"Total\"]/sheets_WeeksDF.iloc[-1][\"Dash\"], 2)\n",
    "print(\"Overall hourly pay rate is $\", hourlyRate)\n",
    "\n",
    "# Max and Average values by Delivery, Day, and Week\n",
    "# Some of these values are wrong, because I have not yet implemented a stack value handler\n",
    "print()\n",
    "print(\"Daily Max Values\")\n",
    "print(sheets_DaysDF[[\"Total\", \"Base\", \"Tip\", \"Active\", \"Dash\", \"Deliveries\"]].max())\n",
    "print(sheets_DaysDF[sheets_DaysDF[\"Version\"] == 3][[\"Total Mileage\", \"Active Mileage\"]].max(), \"\\n\" )\n",
    "print(\"Daily Average Values\")\n",
    "print(sheets_DaysDF[[\"Total\", \"Base\", \"Tip\", \"Active\", \"Dash\", \"Deliveries\"]].mean())\n",
    "print(sheets_DaysDF[sheets_DaysDF[\"Version\"] == 3][[\"Total Mileage\", \"Active Mileage\"]].mean(), \"\\n\" )\n",
    "print()\n",
    "# Some of these values are wrong, because I have not yet implemented a stack value handler\n",
    "print(\"Weekly Max Values\")\n",
    "#\n",
    "print(sheets_WeeksDF[[\"Base\", \"Tip\", \"Adj pay\", \"Pre Total\", \"Total\", \"Active\", \"Dash\", \"Deliveries\"]].max())\n",
    "#\n",
    "print(sheets_WeeksDF[sheets_WeeksDF[\"Version\"] == 3][[\"Total Mileage\", \"Active Mileage\"]].max(), \"\\n\")\n",
    "print(\"Weekly Average Values\")\n",
    "#\n",
    "print(sheets_WeeksDF[[\"Base\", \"Tip\", \"Adj pay\", \"Pre Total\", \"Total\", \"Active\", \"Dash\", \"Deliveries\"]].mean())\n",
    "#\n",
    "print(sheets_WeeksDF[sheets_WeeksDF[\"Version\"] == 3][[\"Total Mileage\", \"Active Mileage\"]].mean(), \"\\n\")\n",
    "print()\n",
    "# Some of these values are wrong, because I have not yet implemented a stack value handler\n",
    "print(\"Delivery Max Values\")\n",
    "#\n",
    "print(sheets_DelDF[[\"Distance\", \"Duration\"]].max())\n",
    "print(sheets_result[[\"Total\"]].max(), \"\\n\")\n",
    "print(\"Delivery Average Values\")\n",
    "#\n",
    "print(sheets_DelDF[[\"Distance\", \"Duration\"]].mean())\n",
    "print(sheets_result[[\"Total\"]].mean(), \"\\n\")\n",
    "\n",
    "# END INITIAL EXPLORATION #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05100b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurants rankable by avg or mean base/tip/total\n",
    "# Yes, total is almost always base+tip, but total INCLUDES peak pay, which may be desirable information\n",
    "\n",
    "tempDF = sheets_result.copy()\n",
    "#The line below drops restaurants that only appear once, we may no longer want this to happen\n",
    "#Commented out for now, may make sense to use when considering outliers for averages\n",
    "#tempDF = tempDF[sheets_result[\"Restaurant Name\"].duplicated(keep=False) == True]\n",
    "tempDF = tempDF.groupby([\"Restaurant Name\"]).agg({\"Base\": [np.mean, np.sum], \n",
    "                                                  \"Tip\": [np.mean, np.sum],\n",
    "                                                  \"Total\": [np.mean, np.sum],\n",
    "                                                 'Restaurant Name': 'size'})\n",
    "tempDF.columns = ['Avg Base', 'Sum Base', 'Avg Tip', \n",
    "                  'Sum Tip', 'Avg Total', 'Sum Total', \"Deliveries\"]\n",
    "\n",
    "tempDF = tempDF.round({'Avg Base': 2, \n",
    "                       'Sum Base': 2,\n",
    "                       'Avg Tip': 2,\n",
    "                       'Sum Tip': 2,\n",
    "                       'Avg Total': 2,\n",
    "                       'Sum Total': 2})\n",
    "\n",
    "#Change sort value to see differences\n",
    "tempDF1 = tempDF.index.copy()\n",
    "restaurantsDF = tempDF.copy()\n",
    "tempDF.sort_values(\"Sum Total\", ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#RESTAURANTS DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dollars Earned vs Miles Driven is another factor I wanted to consider in the overall earnings equation\n",
    "\n",
    "# Grab v3 deliveries, drop rows without known distance values\n",
    "# Group by restaurant name, sum on total and distance\n",
    "# Evaluate total pay over total distance as new column\n",
    "# Sort low to high\n",
    "\n",
    "\n",
    "\n",
    "#WE COULD GRAB RESULTS.VERSION==3 INSTEAD \n",
    "tempDF = sheets_result[sheets_result[\"Version\"] == 3].copy()\n",
    "#tempDF = sheets_DelDF.copy()\n",
    "#tempDF = tempDF[tempDF.Distance!=\"-\"]\n",
    "#tempDF[\"Distance\"] = tempDF[\"Distance\"].astype(float)\n",
    "tempDF = tempDF.groupby([\"Restaurant Name\"]).agg({\"Total\": \"sum\", \"Distance\": \"sum\"})\n",
    "tempDF = tempDF.round({'Base': 2, 'Tip': 2})\n",
    "tempDF[\"DollarMilesRatio\"] = tempDF[\"Total\"]/tempDF[\"Distance\"]\n",
    "#tempDF.sort_values(by=\"DollarMilesRatio\")\n",
    "tempDF2 = tempDF.index.copy()\n",
    "\n",
    "restaurantsDF = restaurantsDF.join(tempDF[\"DollarMilesRatio\"])\n",
    "\n",
    "tempDF\n",
    "\n",
    "\n",
    "#JOIN WITH RESTAURANTS DF\n",
    "\n",
    "\n",
    "#160 in total, 115 in v3? so 45 restaurants unique to v1 and 2??\n",
    "\n",
    "#Can I select results where version !=3, get rest count name no\n",
    "#can we select rest from temp1 not in temp2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c47265",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurantsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF3 = tempDF1[~tempDF1.isin(tempDF2)].dropna(how = 'all')\n",
    "tempDF3 #These are the restaurants from just v1, v2\n",
    "#Thought there were too many but, there were actually a lot that haven't been seen since v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e06a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average $/hour and tip amount by day of week\n",
    "# NOTE: This excludes pay adjustments\n",
    "\n",
    "tempDF = sheets_DaysDF.copy()\n",
    "\n",
    "# Can move this up to formatting function calls\n",
    "# Also, check if necessary to convert from string to timestamp to datetime\n",
    "#date_to_timestamp(tempDF, \"Date\")\n",
    "#tempDF[\"Date\"] = pd.to_datetime(tempDF[\"Date\"])\n",
    "\n",
    "#Adds column to DaysDF for the day of the week (Monday, Thursday, etc.)\n",
    "#tempDF[\"Day of Week\"] = tempDF[\"Date\"].dt.day_name()\n",
    "\n",
    "tempDF = tempDF.groupby([\"Day of Week\"]).agg(\n",
    "    {\"Dash\": \"sum\", \n",
    "     \"Total\": [np.mean, np.sum], \n",
    "     \"Tip\": \"sum\", \n",
    "     \"Deliveries\": \"sum\"})\n",
    "tempDF.columns = ['Sum Dash', 'Avg Daily Total', 'Sum Total', 'Sum Tip', 'Sum Deliveries']\n",
    "\n",
    "tempDF[\"$/hr\"] = tempDF[\"Sum Total\"]/tempDF[\"Sum Dash\"]\n",
    "tempDF[\"Avg Delivery Tip\"] = tempDF[\"Sum Tip\"]/tempDF[\"Sum Deliveries\"]\n",
    "tempDF[\"Avg Delivery Total\"] = tempDF[\"Sum Total\"]/tempDF[\"Sum Deliveries\"]\n",
    "\n",
    "tempDF = tempDF.round({'Sum Dash': 2, \n",
    "                       'Avg Total': 2, \n",
    "                       'Sum Total':2, \n",
    "                       'Sum Tip': 2,\n",
    "                       '$/hr': 2,\n",
    "                       'AvgTip': 2,\n",
    "                       'AvgTotal':2})\n",
    "\n",
    "display(tempDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db03e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
